---
---

@article{2103.11251v2,
  author        = {Cynthia Rudin and Chaofen Chen and Zhi Chen and Haiyang Huang and Lesia Semenova and Chudi Zhong},
  title         = {Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges},
  eprint        = {2103.11251v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel 
                    common misunderstandings that dilute the importance of this crucial topic. We also identify
                    10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are
                    recent problems that have arisen in the last few years. These problems are: (1) Optimizing
                    sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing
                    constraints into generalized additive models to encourage sparsity and better interpretability;
                    (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data
                    visualization; (8) Machine learning models that can incorporate physics and other generative
                    or causal constraints; (9) Characterization of the “Rashomon set” of good models; and (10)
                    Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians
                    and computer scientists interested in working in interpretable machine learning},
  year          = {2021},
  month         = {July},
  url           = {https://arxiv.org/abs/2103.11251v2},
  file          = {2103.11251v2.pdf},
  eprintnover   = {2103.11251}
}

@article{2209.08040v2,
  author        = {Rui Xin and Chudi Zhong and Zhi Chen and Takuya Takagi and Margo Seltzer and Cynthia Rudin},
  title         = {Exploring the Whole Rashomon Set of Sparse Decision Trees},
  eprint        = {2209.08040v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {In any given machine learning problem, there might be many models that explain
                    the data almost equally well. However, most learning algorithms return only one
                    of these models, leaving practitioners with no practical way to explore alternative
                    models that might have desirable properties beyond what could be expressed by
                    a loss function. The Rashomon set is the set of these all almost-optimal models.
                    Rashomon sets can be large in size and complicated in structure, particularly
                    for highly nonlinear function classes that allow complex interaction terms, such
                    as decision trees. We provide the first technique for completely enumerating the
                    Rashomon set for sparse decision trees; in fact, our work provides the first complete
                    enumeration of any Rashomon set for a non-trivial problem with a highly nonlinear
                    discrete function class. This allows the user an unprecedented level of control over
                    model choice among all models that are approximately equally good. We represent
                    the Rashomon set in a specialized data structure that supports efficient querying
                    and sampling. We show three applications of the Rashomon set: 1) it can be used to
                    study variable importance for the set of almost-optimal trees (as opposed to a single
                    tree), 2) the Rashomon set for accuracy enables enumeration of the Rashomon sets
                    for balanced accuracy and F1-score, and 3) the Rashomon set for a full dataset
                    can be used to produce Rashomon sets constructed with only subsets of the data
                    set. Thus, we are able to examine Rashomon sets across problems with a new lens,
                    enabling users to choose models rather than be at the mercy of an algorithm that
                    produces only a single model.},
  year          = {2022},
  month         = {Oct},
  url           = {https://arxiv.org/abs/2209.08040v2},
  file          = {2209.08040v2.pdf},
  eprintnover   = {2209.08040}
}

@article{2006.08690v4,
  author        = {Jimmy Lin and Chudi Zhong and Diane Hu and Cynthia Rudin and Margo Seltzer},
  title         = {Generalized and Scalable Optimal Sparse Decision Trees},
  eprint        = {2006.08690v4},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {Decision tree optimization is notoriously difficult from a computational perspective but essential for the field of interpretable machine learning. Despite efforts over the past 40 years, only
                    recently have optimization breakthroughs been
                    made that have allowed practical algorithms to
                    find optimal decision trees. These new techniques have the potential to trigger a paradigm
                    shift where it is possible to construct sparse decision trees to efficiently optimize a variety of objective functions without relying on greedy splitting and pruning heuristics that often lead to suboptimal solutions. The contribution in this work
                    is to provide a general framework for decision
                    tree optimization that addresses the two significant open problems in the area: treatment of imbalanced data and fully optimizing over continuous variables. We present techniques that produce optimal decision trees over a variety of objectives including F-score, AUC, and partial area
                    under the ROC convex hull. We also introduce
                    a scalable algorithm that produces provably optimal results in the presence of continuous variables and speeds up decision tree construction by
                    several orders of magnitude relative to the stateof-the art.},
  year          = {2022},
  month         = {Nov},
  url           = {https://arxiv.org/abs/2006.08690},
  file          = {2006.08690.pdf},
  eprintnover   = {2006.08690v4}
}

@article{2006.08690v4,
  author        = {Hayden McTavish and Chudi Zhong and Reto Achermann and Ilias Karimalis and Jacques Chen and Cynthia Rudin and Margo Seltzer},
  title         = {Fast Sparse Decision Tree Optimization via Reference Ensembles},
  eprint        = {2112.00798v7},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {Sparse decision tree optimization has been one of the most fundamental problems in AI since its inception and is a challenge
                    at the core of interpretable machine learning. Sparse decision
                    tree optimization is computationally hard, and despite steady
                    effort since the 1960’s, breakthroughs have been made on
                    the problem only within the past few years, primarily on the
                    problem of finding optimal sparse decision trees. However,
                    current state-of-the-art algorithms often require impractical
                    amounts of computation time and memory to find optimal or
                    near-optimal trees for some real-world datasets, particularly
                    those having several continuous-valued features. Given that the
                    search spaces of these decision tree optimization problems are
                    massive, can we practically hope to find a sparse decision tree
                    that competes in accuracy with a black box machine learning
                    model? We address this problem via smart guessing strategies
                    that can be applied to any optimal branch-and-bound-based
                    decision tree algorithm. The guesses come from knowledge
                    gleaned from black box models. We show that by using these
                    guesses, we can reduce the run time by multiple orders of
                    magnitude while providing bounds on how far the resulting
                    trees can deviate from the black box’s accuracy and expressive
                    power. Our approach enables guesses about how to bin continuous features, the size of the tree, and lower bounds on the
                    error for the optimal decision tree. Our experiments show that
                    in many cases we can rapidly construct sparse decision trees
                    that match the accuracy of black box models. To summarize:
                    when you are having trouble optimizing, just guess.},
  year          = {2022},
  month         = {July},
  url           = {https://arxiv.org/abs/2112.00798},
  file          = {2112.00798.pdf},
  eprintnover   = {2112.00798v7}
}

@article{378995.379232,
  author        = {Emery D. Berger and Kathryn S. McKinley and Robert D. Blumofe and Paul R. Wilson},
  title         = {Hoard: A Scalable Memory Allocator for Multithreaded Applications},
  eprint        = {378995.379232},
  archiveprefix = {},
  primaryclass  = {},
  abstract      = {Parallel, multithreaded C and C++ programs such as web servers,
                    database managers, news servers, and scientific applications are becoming increasingly prevalent. For these applications, the memory
                    allocator is often a bottleneck that severely limits program performance and scalability on multiprocessor systems. Previous allocators suffer from problems that include poor performance and scalability, and heap organizations that introduce false sharing. Worse,
                    many allocators exhibit a dramatic increase in memory consumption when confronted with a producer-consumer pattern of object
                    allocation and freeing. This increase in memory consumption can
                    range from a factor of P (the number of processors) to unbounded
                    memory consumption.
                    This paper introduces Hoard, a fast, highly scalable allocator
                    that largely avoids false sharing and is memory efficient. Hoard
                    is the first allocator to simultaneously solve the above problems.
                    Hoard combines one global heap and per-processor heaps with a
                    novel discipline that provably bounds memory consumption and
                    has very low synchronization costs in the common case. Our results on eleven programs demonstrate that Hoard yields low average fragmentation and improves overall program performance over
                    the standard Solaris allocator by up to a factor of 60 on 14 processors, and up to a factor of 18 over the next best allocator we tested.},
  year          = {2000},
  month         = {Nov},
  url           = {https://dl.acm.org/doi/10.1145/378995.379232},
  file          = {https://dl.acm.org/doi/pdf/10.1145/378995.379232},
  eprintnover   = {378995.379232}
}