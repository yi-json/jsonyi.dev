<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://jsonyi.dev/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jsonyi.dev/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-30T20:56:27+00:00</updated><id>https://jsonyi.dev/feed.xml</id><title type="html">blank</title><subtitle>A portfolio for Jason Yi. </subtitle><entry><title type="html">Building Synapse: A Borg-like Cluster Scheduler for AI</title><link href="https://jsonyi.dev/blog/2025/synapse/" rel="alternate" type="text/html" title="Building Synapse: A Borg-like Cluster Scheduler for AI"/><published>2025-11-30T00:00:00+00:00</published><updated>2025-11-30T00:00:00+00:00</updated><id>https://jsonyi.dev/blog/2025/synapse</id><content type="html" xml:base="https://jsonyi.dev/blog/2025/synapse/"><![CDATA[<p>Standard web servers are happy to start one by one. If you ask for 50 servers and get 40, your website still works.</p> <p><strong>AI is different.</strong> If you are training a massive model (like Llama 3) across 64 GPUs, and you can only get 63, the job <strong>cannot start</strong>. Standard schedulers will reserve those 63 GPUs and let them sit idle while waiting for the last one. This wastes millions of dollars in compute time.</p> <p>To solve this, I built <strong>Synapse</strong>, a distributed cluster orchestrator that implements <strong>Gang Scheduling</strong> (All-or-Nothing allocation) to maximize resource utilization for AI workloads.</p> <h2 id="architecture-mimicking-borg">Architecture: Mimicking Borg</h2> <p>Synapse follows the classic Google <strong>Borg</strong> architecture (the predecessor to Kubernetes), consisting of three distinct components:</p> <ol> <li><strong>The Brain (Scheduler):</strong> Holds the “state of the world.” It knows which nodes are free, which are busy, and where they are physically located.</li> <li><strong>The Muscle (Worker):</strong> A lightweight agent on every machine. It listens for orders (“Start Job 50”) and executes them using my custom <a href="/blog/2025/carapace/">Carapace runtime</a>.</li> <li><strong>The Interface (CLI):</strong> A <code class="language-plaintext highlighter-rouge">kubectl</code>-like tool for submitting jobs.</li> </ol> <h3 id="topology-awareness">Topology Awareness</h3> <p>In a massive data center, the speed of light matters. If <code class="language-plaintext highlighter-rouge">Node A</code> and <code class="language-plaintext highlighter-rouge">Node B</code> are on the same rack, they can talk instantly. If they are on opposite sides of the building, <strong>latency destroys performance</strong>.</p> <p>Synapse isn’t just looking for <em>any</em> free GPUs. It tries to find GPUs on the <strong>same rack</strong> (or simulated grouping) to maximize training speed.</p> <h2 id="the-core-problem-gang-scheduling">The Core Problem: Gang Scheduling</h2> <p>The biggest challenge in AI infrastructure is ensuring that a job gets <em>all</em> its requested resources at the exact same time.</p> <p>I implemented a <strong>Gang Scheduler</strong> loop that runs every second. It iterates through the pending job queue and performs an atomic check:</p> <div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// Gang Scheduling: either the job gets ALL its resources, or it waits</span>
<span class="k">func</span> <span class="p">(</span><span class="n">c</span> <span class="o">*</span><span class="n">InMemoryCluster</span><span class="p">)</span> <span class="n">Schedule</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">c</span><span class="o">.</span><span class="n">mu</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
    <span class="k">defer</span> <span class="n">c</span><span class="o">.</span><span class="n">mu</span><span class="o">.</span><span class="n">Unlock</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">job</span> <span class="o">:=</span> <span class="k">range</span> <span class="n">c</span><span class="o">.</span><span class="n">jobQueue</span> <span class="p">{</span>
        <span class="k">if</span> <span class="n">job</span><span class="o">.</span><span class="n">Status</span> <span class="o">!=</span> <span class="n">JobPending</span> <span class="p">{</span> <span class="k">continue</span> <span class="p">}</span>

        <span class="c">// 1. Scan: Can we satisfy this job's requirements?</span>
        <span class="n">neededCPU</span> <span class="o">:=</span> <span class="n">job</span><span class="o">.</span><span class="n">MinCPU</span>
        <span class="n">candidateNodes</span> <span class="o">:=</span> <span class="p">[]</span><span class="o">*</span><span class="n">Node</span><span class="p">{}</span>

        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">node</span> <span class="o">:=</span> <span class="k">range</span> <span class="n">c</span><span class="o">.</span><span class="n">nodes</span> <span class="p">{</span>
            <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">AvailableCPU</span><span class="p">()</span> <span class="o">&gt;</span> <span class="m">0</span> <span class="p">{</span>
                <span class="n">candidateNodes</span> <span class="o">=</span> <span class="nb">append</span><span class="p">(</span><span class="n">candidateNodes</span><span class="p">,</span> <span class="n">node</span><span class="p">)</span>
                <span class="n">neededCPU</span> <span class="o">-=</span> <span class="n">node</span><span class="o">.</span><span class="n">AvailableCPU</span><span class="p">()</span>
            <span class="p">}</span>
        <span class="p">}</span>

        <span class="c">// 2. Decision: If we can't find enough, WAIT.</span>
        <span class="c">// Do not partially allocate resources.</span>
        <span class="k">if</span> <span class="n">neededCPU</span> <span class="o">&gt;</span> <span class="m">0</span> <span class="p">{</span>
            <span class="k">continue</span>
        <span class="p">}</span>

        <span class="c">// 3. Commit: We have enough. Atomically claim resources.</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">node</span> <span class="o">:=</span> <span class="k">range</span> <span class="n">candidateNodes</span> <span class="p">{</span>
            <span class="c">// ... update node state ...</span>
        <span class="p">}</span>
        <span class="n">job</span><span class="o">.</span><span class="n">Status</span> <span class="o">=</span> <span class="n">JobScheduled</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>This prevents deadlocks where multiple jobs are holding onto partial resources, waiting for each other to finish.</p> <h2 id="engineering-challenges">Engineering Challenges</h2> <h3 id="1-the-split-brain--bi-directional-grpc">1. The “Split Brain” &amp; Bi-Directional gRPC</h3> <p>In a typical web app, the client talks to the server. But in a cluster, who is the client?</p> <ul> <li><strong>Worker -&gt; Master:</strong> The worker needs to send Heartbeats (“I’m alive”).</li> <li><strong>Master -&gt; Worker:</strong> The master needs to send Commands (“Start Job”).</li> </ul> <p>I solved this by making the Worker act as <strong>both a Client and a Server</strong>.</p> <ul> <li>It runs a background goroutine to push Heartbeats to the Master.</li> <li>It blocks the main thread on a <code class="language-plaintext highlighter-rouge">grpcServer.Serve(lis)</code> call to listen for incoming <code class="language-plaintext highlighter-rouge">StartJob</code> commands.</li> </ul> <h3 id="2-fault-tolerance-the-reaper">2. Fault Tolerance: The Reaper</h3> <p>Distributed systems must assume failure. I implemented a <strong>Reaper</strong> process in the Scheduler that scans for “silent” nodes.</p> <p>If a node hasn’t sent a heartbeat in 10 seconds, the Reaper marks it as <code class="language-plaintext highlighter-rouge">DEAD</code>. This is critical for the Gang Scheduler—if a node dies, we need to know immediately so we don’t try to schedule new jobs on it.</p> <div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// The Reaper Loop</span>
<span class="k">go</span> <span class="k">func</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">ticker</span> <span class="o">:=</span> <span class="n">time</span><span class="o">.</span><span class="n">NewTicker</span><span class="p">(</span><span class="m">5</span> <span class="o">*</span> <span class="n">time</span><span class="o">.</span><span class="n">Second</span><span class="p">)</span>
    <span class="k">for</span> <span class="k">range</span> <span class="n">ticker</span><span class="o">.</span><span class="n">C</span> <span class="p">{</span>
        <span class="c">// Mark nodes as DEAD if they missed heartbeats</span>
        <span class="n">deadIDs</span> <span class="o">:=</span> <span class="n">clusterManager</span><span class="o">.</span><span class="n">MarkDeadNodes</span><span class="p">(</span><span class="m">10</span> <span class="o">*</span> <span class="n">time</span><span class="o">.</span><span class="n">Second</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">id</span> <span class="o">:=</span> <span class="k">range</span> <span class="n">deadIDs</span> <span class="p">{</span>
            <span class="n">log</span><span class="o">.</span><span class="n">Printf</span><span class="p">(</span><span class="s">"REAPER: Node %s is DEAD"</span><span class="p">,</span> <span class="n">id</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}()</span>
</code></pre></div></div> <h3 id="3-bridging-go-and-rust">3. Bridging Go and Rust</h3> <p>Synapse is written in <strong>Go</strong> (for its excellent concurrency primitives), but the container runtime (Carapace) is written in <strong>Rust</strong> (for low-level Linux control).</p> <p>To bridge them, I used Go’s <code class="language-plaintext highlighter-rouge">os/exec</code> package to perform a <strong>Fork/Exec</strong> operation. The Go worker acts as the supervisor, forking a new process to run the Rust binary, which then sets up the Namespaces and Cgroups for the container.</p> <div class="language-go highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// Go Worker Code</span>
<span class="n">cmd</span> <span class="o">:=</span> <span class="n">exec</span><span class="o">.</span><span class="n">Command</span><span class="p">(</span><span class="s">"./carapace"</span><span class="p">,</span> <span class="s">"run"</span><span class="p">,</span> <span class="n">req</span><span class="o">.</span><span class="n">JobId</span><span class="p">,</span> <span class="n">req</span><span class="o">.</span><span class="n">Image</span><span class="p">)</span>
<span class="n">cmd</span><span class="o">.</span><span class="n">Stdout</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">Stdout</span>
<span class="n">cmd</span><span class="o">.</span><span class="n">Stderr</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">Stderr</span>
<span class="n">err</span> <span class="o">:=</span> <span class="n">cmd</span><span class="o">.</span><span class="n">Run</span><span class="p">()</span> <span class="c">// Blocks until the container finishes</span>
</code></pre></div></div> <h3 id="4-the-rootfs-problem">4. The RootFS Problem</h3> <p>Since Carapace is a custom runtime, it doesn’t pull images from Docker Hub automatically. If you pass <code class="language-plaintext highlighter-rouge">ubuntu:latest</code>, it fails because it looks for a folder on disk.</p> <p>To make this work, I had to manually export a valid <strong>Root Filesystem (RootFS)</strong> from Docker:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create a dummy container</span>
docker create <span class="nt">--name</span> temp-export ubuntu:latest
<span class="c"># Export the filesystem to a folder</span>
docker <span class="nb">export </span>temp-export | <span class="nb">tar</span> <span class="nt">-x</span> <span class="nt">-C</span> my-rootfs
</code></pre></div></div> <p>This gave Synapse a real environment to chroot into, allowing it to run actual Linux commands.</p> <h2 id="conclusion">Conclusion</h2> <p>Building Synapse gave me a deep appreciation for the complexity of schedulers like Kubernetes and Borg. It’s not just about finding a free server; it’s about managing state, handling partial failures, and ensuring that expensive hardware isn’t sitting idle.</p> <p>Check out the full source code on <a href="https://github.com/yi-json/synapse">GitHub</a>.</p>]]></content><author><name></name></author><category term="systems"/><category term="dist-sys"/><category term="go"/><category term="kubernetes"/><category term="borg"/><category term="ai-infra"/><summary type="html"><![CDATA[Solving the "Gang Scheduling" problem for massive AI training jobs using Go and gRPC.]]></summary></entry><entry><title type="html">Gemini 3.0: The Triumph of Vertical Integration</title><link href="https://jsonyi.dev/blog/2025/gemini-3-tpu/" rel="alternate" type="text/html" title="Gemini 3.0: The Triumph of Vertical Integration"/><published>2025-11-29T00:00:00+00:00</published><updated>2025-11-29T00:00:00+00:00</updated><id>https://jsonyi.dev/blog/2025/gemini-3-tpu</id><content type="html" xml:base="https://jsonyi.dev/blog/2025/gemini-3-tpu/"><![CDATA[<p>The release of <strong>Gemini 3.0</strong> isn’t just a milestone for model quality; it’s a victory lap for Google’s custom infrastructure stack. While the rest of the industry scrambles for GPUs, Google has quietly demonstrated that owning the entire vertical—from the silicon (TPU) to the compiler (XLA) to the framework (JAX) is the ultimate competitive advantage. Read more about it on Google Cloud’s blog, <a href="&quot;https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer&quot;">“Introducing Cloud TPU v5p and AI Hypercomputer”</a></p> <h2 id="the-silicon-tpuv6-and-ocs">The Silicon: TPUv6 and OCS</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/content/gemini_tpu.webp-480.webp 480w,/assets/img/posts/content/gemini_tpu.webp-800.webp 800w,/assets/img/posts/content/gemini_tpu.webp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/posts/content/gemini_tpu.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>At the heart of Gemini 3.0 is the <strong>TPUv6</strong>. Unlike general‑purpose GPUs, TPUs are domain‑specific architectures (DSAs) designed for one thing: massive matrix multiplication. <a href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm">Google’s official TPU architecture page details the MXU and HBM design</a></p> <p>The real magic, however, isn’t just the compute; it’s the <strong>networking</strong>. Gemini 3.0 was likely trained on pods connected via <strong>Optical Circuit Switches (OCS)</strong>. This allows for dynamic topology reconfiguration. If a rack fails, the network can physically re‑route light paths in milliseconds, bypassing the fault without stalling the training run. This level of resilience is unheard of in standard InfiniBand clusters.</p> <h2 id="the-software-jax-and-xla">The Software: JAX and XLA</h2> <p>Hardware is useless without a compiler. <strong>XLA (Accelerated Linear Algebra)</strong> is the secret weapon here. It fuses operations to minimize memory bandwidth usage—the bottleneck of modern LLMs. <a href="https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html">The JAX documentation’s “Thinking in JAX” guide explains XLA optimizations</a></p> <p>When you write code in <strong>JAX</strong>, you aren’t just executing kernels; you are describing a computation graph that XLA optimizes globally. For Gemini 3.0, this meant:</p> <ul> <li><strong>SPMD (Single Program, Multiple Data):</strong> Writing code for one device and letting the compiler shard it across 50,000 chips. <a href="https://cloud.google.com/blog/topics/developers-practitioners/scaling-jax-tpus">Google Cloud Blog’s “Scaling JAX on TPUs” details this approach</a></li> <li><strong>Pjit:</strong> Automatic partitioning of tensor dimensions.</li> </ul> <p>Here is a simplified example of what this looks like in practice:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">from</span> <span class="n">jax.sharding</span> <span class="kn">import</span> <span class="n">Mesh</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">,</span> <span class="n">NamedSharding</span>

<span class="c1"># 1. Define the physical mesh of TPUs (e.g., 4x4 grid)
</span><span class="n">mesh</span> <span class="o">=</span> <span class="nc">Mesh</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="nf">devices</span><span class="p">(),</span> <span class="p">(</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">))</span>

<span class="c1"># 2. Define how data should be partitioned
# 'x' dimension of data maps to 'x' dimension of mesh
# 'y' dimension of data maps to 'y' dimension of mesh
</span><span class="n">sharding</span> <span class="o">=</span> <span class="nc">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="nc">PartitionSpec</span><span class="p">(</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">))</span>

<span class="c1"># 3. Write "single device" code, let the compiler shard it
</span><span class="nd">@jax.jit</span><span class="p">(</span><span class="n">out_shardings</span><span class="o">=</span><span class="n">sharding</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># The compiler automatically handles communication across the 50,000 chips
</span></code></pre></div></div> <h2 id="why-this-matters">Why This Matters</h2> <p>For systems engineers, Gemini 3.0 is a manifesto for vertical integration. It demonstrates that the next frontier in AI scaling isn’t just about bigger models, but about the tight coupling of silicon, network, and compiler.</p> <p>In a post-Moore’s Law era, you can’t just buy performance; you have to architect it. Google’s ability to co-design the TPU for the OCS and XLA for the TPU is what allows them to push the boundaries of what’s physically possible.</p>]]></content><author><name></name></author><category term="systems"/><category term="ai-infra"/><category term="tpu"/><category term="google"/><category term="dist-sys"/><category term="gemini"/><summary type="html"><![CDATA[Analyzing the infrastructure behind Gemini 3.0—how TPUv6 and XLA enabled the next leap in model scale.]]></summary></entry><entry><title type="html">Building Carapace: A Container Runtime from Scratch in Rust</title><link href="https://jsonyi.dev/blog/2025/carapace/" rel="alternate" type="text/html" title="Building Carapace: A Container Runtime from Scratch in Rust"/><published>2025-11-23T00:00:00+00:00</published><updated>2025-11-23T00:00:00+00:00</updated><id>https://jsonyi.dev/blog/2025/carapace</id><content type="html" xml:base="https://jsonyi.dev/blog/2025/carapace/"><![CDATA[<p>Containers feel like magic. You run a command, and suddenly your process is isolated in its own little world. But under the hood, there is no magic, just Linux primitives.</p> <p>In this post, I’ll walk through how I built <strong>Carapace</strong>, a lightweight container runtime written in Rust. I’ll explain the core technologies that make containers possible, such as<strong>Namespaces</strong>, <strong>Cgroups</strong>, and <strong>Chroot</strong>, and how Rust’s safety guarantees make it the perfect language for systems programming.</p> <h2 id="the-core-primitives">The Core Primitives</h2> <p>A “container” is effectively a process that is lied to by the kernel. We achieve this deception using three main tools:</p> <ol> <li><strong>Namespaces:</strong> Isolate <em>what</em> a process can see (PIDs, mounts, network).</li> <li><strong>Cgroups (Control Groups):</strong> Limit <em>how much</em> a process can use (CPU, RAM).</li> <li><strong>Chroot:</strong> Change <em>where</em> the process thinks the root of the filesystem is.</li> </ol> <h2 id="phase-1-the-skeleton-namespaces">Phase 1: The Skeleton (Namespaces)</h2> <p>The first step is to create a process that is “disconnected” from the host. We use the <code class="language-plaintext highlighter-rouge">unshare</code> syscall to create new “rooms” for the Hostname (UTS) and Process IDs (PID).</p> <p>I designed Carapace with a “Parent-Child” architecture:</p> <ol> <li><strong>Parent:</strong> Sets up isolation and spawns the child.</li> <li><strong>Child:</strong> Configures the environment (hostname, filesystem) and executes the user’s command.</li> </ol> <div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">use</span> <span class="nn">nix</span><span class="p">::</span><span class="nn">sched</span><span class="p">::{</span><span class="n">unshare</span><span class="p">,</span> <span class="n">CloneFlags</span><span class="p">};</span>
<span class="k">use</span> <span class="nn">std</span><span class="p">::</span><span class="nn">process</span><span class="p">::{</span><span class="n">Command</span><span class="p">,</span> <span class="n">Stdio</span><span class="p">};</span>

<span class="k">fn</span> <span class="nf">run</span><span class="p">(</span><span class="n">cmd</span><span class="p">:</span> <span class="nb">String</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">String</span><span class="o">&gt;</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nb">Result</span><span class="o">&lt;</span><span class="p">()</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="nd">println!</span><span class="p">(</span><span class="s">"Parent: Setting up isolation..."</span><span class="p">);</span>

    <span class="c1">// 1. Create new "rooms" for Hostname (UTS) and PIDs</span>
    <span class="k">let</span> <span class="n">flags</span> <span class="o">=</span> <span class="nn">CloneFlags</span><span class="p">::</span><span class="n">CLONE_NEWUTS</span> <span class="p">|</span> <span class="nn">CloneFlags</span><span class="p">::</span><span class="n">CLONE_NEWPID</span> <span class="p">|</span> <span class="nn">CloneFlags</span><span class="p">::</span><span class="n">CLONE_NEWNS</span><span class="p">;</span>
    <span class="nf">unshare</span><span class="p">(</span><span class="n">flags</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>

    <span class="c1">// 2. Re-Exec: spawn a copy of OURSELVES into those new rooms</span>
    <span class="k">let</span> <span class="k">mut</span> <span class="n">child</span> <span class="o">=</span> <span class="nn">Command</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="s">"/proc/self/exe"</span><span class="p">)</span>
        <span class="nf">.arg</span><span class="p">(</span><span class="s">"child"</span><span class="p">)</span> <span class="c1">// Call our internal "child" subcommand</span>
        <span class="nf">.arg</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span>
        <span class="nf">.args</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="nf">.spawn</span><span class="p">()</span><span class="o">?</span><span class="p">;</span>

    <span class="n">child</span><span class="nf">.wait</span><span class="p">()</span><span class="o">?</span><span class="p">;</span>
    <span class="nf">Ok</span><span class="p">(())</span>
<span class="p">}</span>
</code></pre></div></div> <p>By re-executing <code class="language-plaintext highlighter-rouge">/proc/self/exe</code>, we allow the child process to start fresh inside the new namespaces.</p> <h2 id="phase-2-the-jail-filesystem">Phase 2: The Jail (Filesystem)</h2> <p>Isolation isn’t enough if the container can still see the host’s files. We need to trap the process in a separate root filesystem (I used Alpine Linux for this).</p> <p>We use <code class="language-plaintext highlighter-rouge">chroot</code> to change the root directory and <code class="language-plaintext highlighter-rouge">chdir</code> to ensure the process is physically inside the new jail.</p> <div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">use</span> <span class="nn">nix</span><span class="p">::</span><span class="nn">unistd</span><span class="p">::{</span><span class="n">chroot</span><span class="p">,</span> <span class="n">chdir</span><span class="p">};</span>

<span class="k">fn</span> <span class="nf">child</span><span class="p">(</span><span class="n">cmd</span><span class="p">:</span> <span class="nb">String</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">String</span><span class="o">&gt;</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nb">Result</span><span class="o">&lt;</span><span class="p">()</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="nd">println!</span><span class="p">(</span><span class="s">"Child: Entering chroot jail..."</span><span class="p">);</span>
    
    <span class="c1">// 1. The Lock: Restrict filesystem access to the 'rootfs' folder</span>
    <span class="nf">chroot</span><span class="p">(</span><span class="s">"rootfs"</span><span class="p">)</span><span class="o">?</span><span class="p">;</span> 
    
    <span class="c1">// 2. The Entry: Move current working directory into the new root</span>
    <span class="nf">chdir</span><span class="p">(</span><span class="s">"/"</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>

    <span class="c1">// 3. Mount /proc so tools like 'ps' work</span>
    <span class="nf">mount</span><span class="p">(</span>
        <span class="nf">Some</span><span class="p">(</span><span class="s">"proc"</span><span class="p">),</span>
        <span class="s">"/proc"</span><span class="p">,</span>
        <span class="nf">Some</span><span class="p">(</span><span class="s">"proc"</span><span class="p">),</span>
        <span class="nn">MsFlags</span><span class="p">::</span><span class="nf">empty</span><span class="p">(),</span>
        <span class="nn">None</span><span class="p">::</span><span class="o">&lt;&amp;</span><span class="nb">str</span><span class="o">&gt;</span>
    <span class="p">)</span><span class="o">?</span><span class="p">;</span>

    <span class="c1">// ... execute user command ...</span>
    <span class="nf">Ok</span><span class="p">(())</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Crucial Detail:</strong> After <code class="language-plaintext highlighter-rouge">chroot</code>, the <code class="language-plaintext highlighter-rouge">/proc</code> directory is empty. We must manually mount the <code class="language-plaintext highlighter-rouge">proc</code> pseudo-filesystem so that tools like <code class="language-plaintext highlighter-rouge">ps</code> can read process information from the kernel.</p> <h2 id="phase-3-resource-limits-cgroups">Phase 3: Resource Limits (Cgroups)</h2> <p>To prevent a container from hogging the entire machine’s CPU or memory, we use <strong>Control Groups (Cgroups)</strong>.</p> <p>I implemented a “Sandwich Pattern” to manage the lifecycle of these resources:</p> <ol> <li><strong>Setup:</strong> Create the Cgroup and set limits <em>before</em> the child starts.</li> <li><strong>Run:</strong> The child process joins the Cgroup.</li> <li><strong>Cleanup:</strong> Delete the Cgroup <em>after</em> the child exits.</li> </ol> <div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">fn</span> <span class="nf">run</span><span class="p">(</span><span class="n">cmd</span><span class="p">:</span> <span class="nb">String</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">String</span><span class="o">&gt;</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nb">Result</span><span class="o">&lt;</span><span class="p">()</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="c1">// 1. Setup: Build the "cage" first</span>
    <span class="nf">setup_cgroups</span><span class="p">()</span><span class="o">?</span><span class="p">;</span>

    <span class="c1">// ... start child process ...</span>

    <span class="n">child</span><span class="nf">.wait</span><span class="p">()</span><span class="o">?</span><span class="p">;</span>
    
    <span class="c1">// 2. Cleanup: Remove the "cage" to prevent memory leaks</span>
    <span class="nf">clean_cgroups</span><span class="p">()</span><span class="o">?</span><span class="p">;</span>

    <span class="nf">Ok</span><span class="p">(())</span>
<span class="p">}</span>
</code></pre></div></div> <p>If we didn’t clean up, the Cgroup directories would persist in <code class="language-plaintext highlighter-rouge">/sys/fs/cgroup/</code>, eventually causing a memory leak in the kernel.</p> <h2 id="why-rust">Why Rust?</h2> <p>Writing a container runtime involves a lot of raw system calls. In C, handling <code class="language-plaintext highlighter-rouge">unshare</code>, <code class="language-plaintext highlighter-rouge">chroot</code>, and memory management manually is a minefield.</p> <p>Rust gives me:</p> <ul> <li><strong><code class="language-plaintext highlighter-rouge">Result&lt;T, E&gt;</code>:</strong> Forces me to handle every syscall failure (no more silent crashes).</li> <li><strong>Safety:</strong> The borrow checker ensures I’m not leaking memory or accessing invalid pointers.</li> <li><strong>FFI:</strong> I even integrated a C++ inspector using Rust’s FFI capabilities to read kernel versions!</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>Building Carapace taught me that containers are a clever composition of Linux features that have existed for years. By implementing them from scratch, you gain a much deeper appreciation for the engineering behind Docker and Kubernetes.</p> <p>Check out the full source code on <a href="https://github.com/yi-json/carapace">GitHub</a>.</p>]]></content><author><name></name></author><category term="low-level"/><category term="rust"/><category term="systems"/><category term="containers"/><category term="linux"/><summary type="html"><![CDATA[A deep dive into Linux Namespaces, Cgroups, and how I built a secure container runtime using Rust.]]></summary></entry></feed>